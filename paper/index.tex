\documentclass{article} % For LaTeX2e
\usepackage[utf8]{inputenc}
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{caption} 
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{Recurrent Batch Normalization}

\author{
Me \\
\And
Nicolas \\
\And
C\'esar \\
\And
Aaron
\texttt{email} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\usepackage{amsfonts}
\usepackage{amsmath}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\act}{f}
\newcommand{\ewprod}{\odot}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\given}{\vert}

\bibliographystyle{plain}

\begin{document}

\maketitle

\begin{abstract}
We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks.
Empirical results show faster convergence and improved generalization on classification, language modeling and speech transcription tasks.
\end{abstract}

\section{Introduction}


%% FIXME redo intro

%% Training deep neural networks by gradient descent is hard due in large part to pathological curvature in the objective function.
%% Approaches to alleviating this problem divide naturally into two categories: smarter optimization algorithms such as
%% Hessian-Free~\cite{hessianfree} and various approximations to natural gradient~\cite{amari} proposed in~\cite{ollivier} and~\cite{KFAC},
%% and model reparameterizations~\cite{efficientbackprop}~\cite{raiko}~\cite{naturalneuralnetworks}.

%Success of first order SGD depends on the  the curvature of the objective that is optimized~\cite{hessianfree} which depends on the networks parametrization~\cite{amari}.


%% It involves normalizing unit preactivations in order to make their distributions easier to control.
%% This greatly improves the training dynamics of deep feedforward neural networks, leading to faster convergence and better generalization.
%% Recently, a related technique called weight normalization~\cite{weightnorm} was introduced, which has a similar effect to batch normalization yet might be easier to carry over to the recurrent setting.
%% [also normprop]
%% However to our knowledge none of these schemes have yet been transferred to RNNs.



%%% This section mix up the argument of jacobian (backprop-through time) and hessian (high curvature) for hard optimization, need to be FIXED
Recurent Neural Networks (RNN)  through the LSTM/GRU re-parametrization~\cite{lstm,cho2014learning} have recently exhibited state-of-art on a wide range of complex sequential problems including speech recognition~\cite{baidu}, machine translation~\cite{bahdanau2014neural} or image/video captionning~\cite{xu2015show,yao2015describing}.
Top-performing models, however, are based on very high-capacity networks that are computationally intensive and lead to time-costly training.

Internal Covariate Shift~\cite{shimodaira2000improving,batchnorm} has been identified as one of the factor slowing down the training process.
It is defined as the layers activations distribution shifting due to the change in network parameters.
The shift in the input of a layer presents a problem because as the layer need to continuously
adapt to the new distribution. Due to its nature, Internal Covariation Shift is especially prononced in very deep network such as RNN where each time-step can be view as the application of a layers.

Batch normalization~\cite{batchnorm} is a recently proposed reparameterization in feedforward network by fixing the distribution of the layer inputs as the training progresses, hence reducing the internal covariate shift.
Batch normalization proposes to whitten the activation of each layer, relying on the observation that networks converge faster when its inputs
have zero mean, unit variance and are uncorrelated~\cite{efficientbackprop}.
In addition, in order to avoid the cost applying a full whittening on the activation,
batch norm approximates the activation mean and standard deviation using mini-batch statistics at each training interation
iteration of training and uses these estimates to normalize the input of the next layer.

However, applying batch normalization in recurrent neural networks (RNNs) has proven to be challenging~\cite{cesar,baidu}.
It has found limited use in stacked RNNs, where the normalization is applied ``vertically'', i.e. to the input of each RNN, but not ``horizontally'' between timesteps, where RNNs are deepest and would most benefit from the reparameterization.
Authors of ~\cite{cesar} hypothesises that applying batch-norm on the hidden-to-hidden transition hurts the training
because of the repeated application of the batchnorm scale and shift parameters could lead to exploding or vanishing gradients.

In contrary to this belief, our findings shows that it is possible to apply batch-normalization to a RNN/LSTM models and that both training and regularization
benefits can be osbserve as well. In this paper, we demonstrate how batch normalization can be used in RNNs to speed up optimization and improve
generalization. In addition, we also shows that batch-normalization provide a way of stabilizing the layer activation function, making it less likely
to saturate, hence easing the gradient backpropagation.


Section~\ref{sec:prerequisites} introduces RNNs and batch normalization in detail.
In Section~\ref{sec:recurrent-batch-normalization} we discuss batch normalization in the recurrent setting.
We show in Section~\ref{sec:experiments} our evaluations of the proposed reparameterization on a variety of tasks.

\section{Prerequisites}
\label{sec:prerequisites}

\subsection{LSTM}

In this section, we review Long Short Term Memory Network  (LSTM) networks~\cite{lstm} which
are a particular type of RNN. An RNN model is applied to a sequence of inputs, which can have variable lengths.
It defines a recurrent hidden state whose activation at each time is dependent on that of the previous time. Specifically, given a sequence
$\mat{X} = ( \vect{x}_1, \vect{x}_2, ... \vect{x}_T )$, the RNN  hidden state at time $t$ is defined as
\begin{eqnarray}
  \mat{h}_t = \phi(\mat{W}_h \vect{h}_{t-1} + \mat{W}_x  \vect{x}_t + \vect{b})
\end{eqnarray}
where $\mat{W}_h \in \reals^{d_h \times d_h}, \mat{W}_x \in \reals^{d_x \times d_h}, \vect{b} \in \reals^{d_h}$ are model parameters.
and $\phi$  is a nonlinear activation function such as $\mathrm{tanh}$


%%% TO Add
Training RNNs using first-order stochastic gradient descent (SGD)t is notoriously difficult as we need to not only control the dynamics of the forward-propagated activations but also those of the back-propagated gradient~\cite{bengio1994learning,hochreiter1991untersuchungen,pascanudifficulty}.
In particular in case of gradient vanishing, errors of a given state will not influence the states far in the past,
 preventing RNN to capture long-term dependencies in the input data.
While the long-term dependencies problem is unsolvable in absolute~\cite{bengio1994learning}, it has been demonstrated that different RNN reparametrization, such the LSTM~\cite{lstm}, GRU~\cite{cho2014learning} or $i$/$u$-RNN~\cite{le2015simple,urnn} can help mitigate the vanishing gradient problem.
We will focus on the LSTM reparametrization in this paper. The LSTM recurrent transition is given by:
\begin{eqnarray}
\left(\begin{array}{ccc}
\tilde{\vect{f}}_t \\
\tilde{\vect{i}}_t \\
\tilde{\vect{o}}_t \\
\tilde{\vect{g}}_t
\end{array}\right)
 &=&
 \mat{W}_h \vect{h}_{t-1} +
 \mat{W}_x \vect{x}_t +
 \vect{b}
 \\
\vect{c}_t &= &\sigma(\tilde{\vect{f}}_t) \ewprod \vect{c}_{t-1} +
\sigma(\tilde{\vect{i}}_t) \ewprod \mathrm{tanh}(\tilde{\vect{g}_t}) \\
\vect{h}_t &= &\sigma(\tilde{\vect{o}}_t) \ewprod \tanh(\vect{c}_t)
\end{eqnarray}
where $\vect{W}_h \in \reals^{d_h \times 4 d_h}, \vect{W}_x \reals^{d_x \times 4 d_h}, \vect{b} \in \reals^{4 d_h}$ and $\sigma$ is the sigmoid non-linearity model parameters. The $\ewprod$ operator denotes the Hadamard product.

LSTM has an  additional memory \emph{cell} state $\vect{c}_t$ whose update is nearly linear which would
allow the gradient to flow more easily through time.
In addition, unlike to the RNN which overwrites its content at each time-step,
an LSTM unit is able to decide whether to keep the existing memory via the introduced gates $\vect{f}_t$, $\vect{i}_t$.
The gate $\vect{o}_t$ defines how much of the memory $\vect{c}_t$ will be ``pass'' to the next timestep.
%%  if the LSTM unit detects an important feature from an input sequence at early stage, it
%% easily carries this information (the existence of the feature) over a long distance, hence, capturing
%% potential long-distance dependencies.
With these gates, the model can control what to forget and what to remember.




\subsection{Batch normalization}

%% Internal Covariate Shift
In deep neural network, an given layer $l$ receives input samples from a distribution that is defined by the layers below $l$.
As we update the network parameters, this distribution can vary during the course of training.
The change in the distributions of layer $l$ inputs presents a problem because the layer need to continuously adapt to the new distribution
This distribution variation is called the Internal Covariate Shift~\cite{shimodaira2000improving}.
Reducing the internal covariate shift should leads  faster training as upper layer does not need to adapt to the distribution change.

Batch Normalization~\cite{batchnorm} proposes a network reparametrization applied to neural network preactivations that takes a step towards reducing
internal covariate shift. Batch normalization] approximates the
whitening by standardizing the preactivation using the statistics of the current batch..
The batch normalizing transform is as follows:

\begin{align}
\mathrm{BN}(\vect{h}; \gamma, \beta) =
  \beta + \gamma
  \frac{\vect{h} -   \widehat{\mathbb{E}}(\vect{h})}
       {       \sqrt{\widehat{\mathrm{Var }}(\vect{h}) + \epsilon}}
\end{align}

where $\vect{h} \in \reals^d$ is the vector of (pre)activations to be normalized, $\gamma \in \reals^d, \beta \in \reals^d$ are model parameters that determine the mean and standard deviation of the normalized activation, and $\epsilon \in \reals$ is a regularization hyperparameter.

At training time, the statistics $\mathbb{E}(\vect{h})$ and $\mathrm{Var}(\vect{h})$ are estimated by the sample mean and sample variance of the current minibatch.
This allows for backpropagation through the statistics, preserving the convergence properties of stochastic gradient descent.
During inference, the statistics are typically estimated based on the entire training set, to produce a deterministic prediction.

Batch normalizing some hidden layer's activation ensures that its mean and variance are determined only by the $\beta$ and $\gamma$ parameters, and not by the parameters of any layer below it.
Applying this on every layer effectively decouples each layer's parameters from those of other layers, leading to a better-conditioned optimization problem.
Deep neural networks trained with batch normalization converge significantly faster and generalize better.

\section{Recurrent batch normalization}
\label{sec:recurrent-batch-normalization}

This section introduces a reparametrization of LSTM that takes advantage
of Batch Normalization. Contrary to previous works~\cite{cesar, baidu}, we leverage of batch normalization in both input-to-hidden and hidden-to-hidden transformation.


We introduce the batch normalizing transform $\mathrm{BN}(\cdot; \gamma, \beta)$ into the LSTM as follows:

\begin{eqnarray}
\left(\begin{array}{ccc}
\tilde{\vect{f}}_t \\
\tilde{\vect{i}}_t \\
\tilde{\vect{o}}_t \\
\tilde{\vect{g}}_t
\end{array}\right)
 &=&
 \mathrm{BN} (\mat{W}_h \vect{h}_{t-1}; \gamma_h, \beta_h) +
 \mathrm{BN} (\mat{W}_x \vect{x}_t    ; \gamma_x, \beta_x) +
 \vect{b}
\\
\vect{c}_t &=& \sigma(\tilde{\vect{f}}_t) \ewprod \vect{c}_{t-1} +
              \sigma(\tilde{\vect{i}}_t) \ewprod \mathrm{tanh}(\tilde{\vect{g}_t}) \\
\vect{h}_t &=& \sigma(\tilde{\vect{o}}_t) \ewprod \tanh(
 \mathrm{BN} (\vect{c}_t; \gamma_c, \beta_c)
)
\end{eqnarray}

Note how we normalize the recurrent term $\mat{W}_h \vect{h}_{t-1}$ and the input term $\mat{W}_x \vect{x}_t$ separately.
This allows the model to control the contribution of the two independently using $\gamma_h$ and $\gamma_x$.
To avoid unnecessary redundancy we set $\beta_h = \beta_x = \vect{0}$, instead relying on the pre-existing parameter vector $\vect{b}$ to account for both biases.
In order to leave the LSTM dynamics intact and preserve the gradient flow through $\vect{c}_t$, we do not apply batch normalization on the cell states.

During training we estimate the statistics needed for batch normalization across the minibatch, and independently for each timestep.
During inference we use estimates obtained by averaging the minibatch estimates over the training set.

%% FIXME develop a little bit
%% Speak about the population statistics



\section{On the Importance of Pre-Activation Variances}

\begin{figure}
  \center%
  \subfigure[
Empirical expected derivative of $\tanh$ nonlinearity as a function of input variance.
High variance causes saturation, which decreases the expected derivative.
]{%
    \label{fig:tanh_grad}
    \includegraphics[width=.45\textwidth]{figures/tanh_grad.pdf}
  }%
  \hspace{2mm}%
  \subfigure[
Gradient flow through a batch-normalized $\tanh$ RNN as a function of $\gamma$.
High variance causes gradient vanishing.
]{%
    \label{fig:rnn_grad_prop}
    \includegraphics[width=.45\textwidth]{figures/rnn_grad_prop.pdf}
  }
  \caption{
Influence of pre-activation variance on gradient propagation.
}
  \label{figure:DCNtradeoff}
\end{figure}

In section we develop the observation that high-variance pre-activations tend to saturate sigmoid nonlinearities.
Figure~\ref{fig:tanh_grad} shows empirically how the expected derivative of the tanh nonlinearity changes with the variance of the argument.
High variance input is more likely to be in the saturating regime where the derivative is small,
whereas low variance input is more likely to be in the linear regime around the origin.

Batch normalization allows for easy control of the pre-activation variance through the $\gamma$ parameters,
but the common practice of normalizing to unit variance causes gradient to vanish in recurrent and very deep feedforward networks.
In Figure~\ref{fig:rnn_grad_prop} we demonstrate how the pre-activation variance impacts gradient propagation in a simple RNN on the sequential MNIST task described in Section~\ref{sec:seqmnist}.
Since backpropagation operates in reverse, the plot is best read from right to left.
The quantity plotted is the norm of the gradient of the loss with respect to the hidden states.
For large values of $\gamma$, the norm quickly goes to zero as gradient is propagated back in time.
For small values of $\gamma$ the norm is nearly constant.

% TODO: make lstm plot with baseline pre-activation variances

\section{Experiments}
\label{sec:experiments}


%%% Find better title
\subsection{Model Specification}


\begin{figure}
\center
\includegraphics[width=0.8\textwidth]{figures/popstat_stationarity.pdf}
\caption{Convergence of population statistics to stationary distributions on the Penn Treebank task. Only a random subset of population statistics is shown.}
\label{fig:popstat_stationarity}
\end{figure}


A RNN can be ``unrolled'' and viewed as a feedforward neural network.
This view suggests that the success of batch normalization in feedforward networks should carry over to the recurrent setting.
However, unrolled RNNs differ from true feedforward networks in several important respects that complicate the application of batch normalization.

\begin{itemize}
\item
The parameters in an unrolled RNN are shared across layers.
This suggests that the statistics used for normalization should be shared as well.
However, we have found that estimating the statistics independently at each timestep works best.
With batch normalization, the activations rapidly converge to a stationary distribution (see Figure~\ref{fig:popstat_stationarity}), so independent estimation does not hurt performance.
Indeed, we find that sharing statistics does hurt performance, as the distribution of activations during the first few timesteps is very different from the stationary distribution.
\\ \item
The depth depends on the length of the input sequence $\mat{X} = (\vect{x}_t)$, which is typically not constant.
This poses a problem if we want to estimate the population statistics for different timesteps independently.
However, thanks to the stationarity of the activation distribution (cf. Figure~\ref{fig:popstat_stationarity}) we can use the estimates of earlier timesteps for later timesteps.
For our experiments we estimate the population statistics separately for each timestep $1, \ldots, T_{max}$ where $T_{max}$ is the length of the longest training sequence.
When at test time we need to generalize beyond $T_{max}$, we use the population statistic of time $T_{max}$ for all time steps following it.

[there is an annoying little detail here: there may be only a single example with length T, and so the population statistic is highly unreliable. in this case i would take an earlier population statistic instead, the exact choice determined by validation. how do we explain this concisely?]
[additionally, we haven't actually dealt with variable-length training sequences yet. this is another technicality that's easily hacked around but people will want to know i guess.]
\\ \item
There is three-way interaction; each layer receives as input not just the activations of the layer below it but also the next element of the input sequence.
We find that we need to normalize these terms separately before adding them up.
This gives the model better control over the relative contribution of the terms.
\\ \item
The initial states $\vect{h}_0$ are independent of the input and thus have zero variance.
This problem is exacerbated in unnatural data such as MNIST and various pathological tasks, where some features are constant across the data.
In the sequential MNIST task in particular, the variance is typically exactly zero for the first hundred or so time steps, as the upper pixels are almost always black.
Normalizing these zero-variance activations involves division by a very small number at many timesteps, which causes the gradient to explode.
We work around this by injecting noise into the initial hidden states.
Although the normalization amplifies the noise to signal level, we find that it does not hurt performance compared to data-dependent ways of initializing the hidden states.
[but is that an artifact of classification at the end?]
\\ \item
Unrolled RNNs are usually much deeper than typical feedforward networks.
Rather than normalizing activations to have standard deviation 1, we normalize to standard deviation 0.1.
This avoids saturating the nonlinearities and enables better flow of gradients.
Indeed, we found that normalizing to unit variance caused the gradient to vanish rapidly.
[yet explode in LSTM...]
\end{itemize}

\subsection{Sequential MNIST}
\label{sec:seqmnist}

\begin{figure}
\center
\includegraphics[width=6.7cm]{figures/unpermuted_valid.pdf}
\includegraphics[width=6.7cm]{figures/permuted_valid.pdf}
\caption{Accuracy on the validation set for the pixel by pixel MNIST classification tasks. The batch-normalized LSTM is able to converge faster relatively to a baseline LSTM.
  Batch-normalized  LSTM also shows some improve generalization on the permuted sequential MNIST that require to preserve long-term memory information.}
\label{fig:seqmnist_valid}
\end{figure}


We evaluate recurrent batch normalization on a modified version of the MNIST dataset, suggested by~\cite{le2015simple}.
Recurrent models are fed MNIST pixels sequentially and have to output at the end of the sequence the corresponding MNIST label.
We consider two sequentall tasks, MNIST and $p$-(ermuted)MNIST, with different pixel ordering, MNIST considers the pixels from left to right, while $p$-MNIST uses and random ordering of the pixels.
As model, we consider a LSTM using $100$ hidden units. A softmax classifier
is applied on the final hidden representation.
LSTM relies on orthogonal initialization for the weight matrix, except for the hidden-to-hidden weight matrix that used identity initiliazation, as we observe better generalization performance with this
initiliaziation for this tasks.
The model is trained using RMSProp~\cite{rmsprop} with learning rate of $10^{-3}$ and a decay rate of $0.9$. We apply gradient clipping at 1 to avoid exploding gradients.

\begin{table}
\center
\small
\begin{tabular}{c|c|c}
  & UnPermuted & Permuted\\
  \hline
  TANH-RNN~\cite{le2015simple} & 35.0 & 35.0\\
  $i$RNN~\cite{le2015simple} & 97.0 & 82.0\\
  $u$RNN~\cite{urnn} & 95.1 & 91.4\\
  $s$TANH-RNN~\cite{zhang2016architectural} & 98.1 & 94.0\\
  \hline
  LSTM & 98.9 & 90.2\\
  BN-LSTM & \textbf{99.0} & \textbf{95.4}\\
\end{tabular}
\caption{Accuracy obtained on the test set for the pixel by pixel MNIST classification tasks}
\label{tab:seqmnist_test}

\end{table}



Figure~\ref{fig:seqmnist_valid} reports the validation accuracy throught the training of both LSTM and batch-normalized LSTM (BN-LSTM). Figure~\ref{fig:seqmnist_valid} clearly shows recurrent batch normalization eases the model optimization, as batch-normalized LSTM converges faster than the baseline LSTM on both tasks.
In addition, we observe that batch-normalized LSTM obtain a significant generalization improvement on $p$-MNIST. It has been highlighted in~\cite{urnn}
that $p$-MNIST creates many longer term dependencies across pixels than in
the original pixel ordering, where a lot of structure is local. A recurrent network therefore needs to characterize dependencies across varying time scales in order to solve this tasks. Results show by using recurrent batch-normalization, we can learn a LSTM that better characterize those long-term dependencies.



Table~\ref{tab:seqmnist_test} reports the test set accuracy of the early stop model for the LSTM and BN-LSTM models using the population statistics. Recurrent batch normalization leds to better test score, especially for $p$-MNIST  where models have to leverage long-term temporal depencies. In addition, Table~\ref{tab:seqmnist_test} shows that our batch normalized LSTM achieves state-of-art on both MNIST and $p$-MNIST.

\subsection{Character-level Penn Treebank}

We evaluate our model on the task of character-level language modeling on the Penn Treebank corpus~\cite{penntreebank}.
We follow the setup of~\cite{krueger}, dividing the training set into nonoverlapping subsequences of length 50.
Our baseline is an LSTM with 1000 units.
We use stochastic gradient descent on minibatches of size 100,
with gradient clipping at 1.0 and step rule determined by RMSProp~\cite{rmsprop}
with learning rate 0.001 and momentum 0.9.
We use orthogonal initialization for all weight matrices.
The setup for the batch-normalized LSTM is the same in all respects except for the introduction of batch normalization.

We were unable to reproduce the baseline reported in~\cite{krueger} and so are unable to compare directly to their results.
However, we show that batch normalizing our best baseline enables it to train faster and generalize better.

%\begin{figure}
%\center
%\includegraphics[width=7cm]{figures/ptb_valid.pdf}
%\caption{Bits-per-character on the validation set for Penn Treebank during training.}
%\label{fig:ptb_valid}
%\end{figure}
%
%\begin{table}
%\center
%\begin{tabular}{c|c}
%  \hline
%  LSTM & BN-LSTM\\
%  \hline
%  NaN & NaN\\
%  \textbf{NaN} & \textbf{NaN}\\
%\end{tabular}
%\caption{Bits-per-character on the Penn Treebank test string.}
%\label{tab:ptb_test}

\subsection{TIMIT}

[do we include this? I'm not opposed to it but not sure what to say]

\subsection{Teaching Machines to Read and Comprehend}

To demonstrate the generality and practical applicability of our
reparameterization, we take an implementation
\footnote{The code can be found at \url{https://github.com/caglar/Attentive_reader}}
of the Attentive Reader model~\cite{attentivereader} and simply replace the LSTM with our
BN-LSTM.
Additionally we try a variant where we also introduce batch
normalization into the attention computations, normalizing each term
going into the tanh nonlinearities.
Without any tweaking, we gain dramatic acceleration of training (Figure~\ref{fig:attr_valid}).

[Caglar will write a blurb on how the subset of data is selected]

\begin{figure}
\center
\includegraphics[width=0.8\textwidth]{figures/attr_valid.pdf}
\caption{
Accuracy on the validation set for the Attentive Reader models on a variant of the CNN QA task~\cite{attentivereader}.
BN-LSTM converges much faster than the baseline LSTM.
Applying batch normalization in the attention computations as well (``BN-everywhere'') converges faster yet.
}
\label{fig:attr_valid}
\end{figure}

\section{Conclusion}

\section*{Acknowledgements}

Experiments were conducted using the Theano~\cite{theano1}~\cite{theano2} and Blocks and Fuel~\cite{blocks} libraries.
[how to acknowledge use of cluster?]
We thank Çağlar Gülçehre for sharing his implementation of Attentive Reader and for helping us with experiments,
and David Krueger, Ishmael Belghazi and Saizheng Zhang for helpful discussions.

\bibliography{index}
\bibliographystyle{nips2015}

\end{document}
